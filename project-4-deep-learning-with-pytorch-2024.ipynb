{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66644,"databundleVersionId":7555250,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3e3c412943bb4adfb3872b95d6d96e46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d25b95b9c534fa3a6896f72cb0f1d16","IPY_MODEL_7104c29b6e514827be16bc3d13cd762d","IPY_MODEL_c31d0cd577d8435780c13748e829a1fe"],"layout":"IPY_MODEL_4a4e47f987d0430fa083aea6ddd7aaaf"}},"6d25b95b9c534fa3a6896f72cb0f1d16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbba94818cae4ddd9625ec934cfe41c9","placeholder":"​","style":"IPY_MODEL_64a7bc80adcd4e78923898e2379aef85","value":"100%"}},"7104c29b6e514827be16bc3d13cd762d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfecc21e178b4337aec3e38f0cea4f67","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c49288be2a91452f8dcb880691626d38","value":100}},"c31d0cd577d8435780c13748e829a1fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c834ac8670a74300893369471b077df2","placeholder":"​","style":"IPY_MODEL_2d1c4e9edd25498d99d9184f3e66eca7","value":" 100/100 [05:08&lt;00:00,  3.05s/it]"}},"4a4e47f987d0430fa083aea6ddd7aaaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbba94818cae4ddd9625ec934cfe41c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64a7bc80adcd4e78923898e2379aef85":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfecc21e178b4337aec3e38f0cea4f67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c49288be2a91452f8dcb880691626d38":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c834ac8670a74300893369471b077df2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d1c4e9edd25498d99d9184f3e66eca7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <font style=\"color:blue\">Project 4: Kaggle Competition - Semantic Segmentation</font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h6>Sr. no.</h6></td> <td><h6>Section</h6></td> <td><h6>Points</h6></td> </tr>\n        <tr><td><h6>1</h6></td> <td><h6>1.1. Dataset Class</h6></td> <td><h6>7</h6></td> </tr>\n        <tr><td><h6>2</h6></td> <td><h6>1.2. Visualize dataset</h6></td> <td><h6>3</h6></td> </tr>\n        <tr><td><h6>3</h6></td> <td><h6>2. Evaluation Metrics</h6></td> <td><h6>10</h6></td> </tr>\n        <tr><td><h6>4</h6></td> <td><h6>3. Model</h6></td> <td><h6>10</h6></td> </tr>\n        <tr><td><h6>5</h6></td> <td><h6>4.1. Train</h6></td> <td><h6>7</h6></td> </tr>\n        <tr><td><h6>6</h6></td> <td><h6>4.2. Inference</h6></td> <td><h6>3</h6></td> </tr>\n        <tr><td><h6>7</h6></td> <td><h6>5. Prepare Submission CSV</h6></td><td><h6>10</h6></td> </tr>\n        <tr><td><h6>8</h6></td> <td><h6>6. Kaggle Profile Link</h6></td> <td><h6>50</h6></td> </tr>\n    </table>\n</div>\n\n---\n\n<h2>Dataset Description </h2>\n<p>The dataset consists of 3,269 images in 12 classes (including background). All images were taken from drones in a variety of scales. Samples are shown below:\n<img src=\"https://github.com/ishann/aeroscapes/blob/master/assets/data_montage.png?raw=true\" width=\"800\" height=\"800\">\n<p>The data was splitted into public train set and private test set which is used for evaluation of submissions.","metadata":{"id":"ibzW-5Jfn_K0"}},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/opencv-pytorch-segmentation-project/\"\nOUTPUT_PATH = \"/kaggle/working/\"","metadata":{"id":"TszH6eB_n_K1","execution":{"iopub.status.busy":"2024-03-04T05:04:32.176536Z","iopub.execute_input":"2024-03-04T05:04:32.176901Z","iopub.status.idle":"2024-03-04T05:04:32.188115Z","shell.execute_reply.started":"2024-03-04T05:04:32.176869Z","shell.execute_reply":"2024-03-04T05:04:32.187185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --quiet albumentations\n!pip install --quiet torch-lr-finder\n!pip install --quiet segmentation-models-pytorch\n!pip install --quiet pytorch_toolbelt\n!pip install --quiet iterative-stratification\n!pip install --quiet torcheval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"QChkUuTNn_K3","outputId":"8e8704c3-1d1e-4d30-db9f-38fdfd5a4e2f","execution":{"iopub.status.busy":"2024-03-04T05:04:32.189722Z","iopub.execute_input":"2024-03-04T05:04:32.190030Z","iopub.status.idle":"2024-03-04T05:05:54.211945Z","shell.execute_reply.started":"2024-03-04T05:04:32.190000Z","shell.execute_reply":"2024-03-04T05:05:54.210866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard Library imports\nimport time\nfrom pathlib import Path\nimport os\nfrom collections import defaultdict\nimport gc\nimport pickle\n\n# External imports\nimport cv2\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MultipleLocator\nimport torchvision.transforms.functional as F\nimport torchvision.transforms as T\nfrom torchvision import models\nfrom torchvision.io import read_image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\nfrom tqdm.autonotebook import tqdm\nfrom albumentations.pytorch import ToTensorV2\nimport albumentations as A\nfrom torch_lr_finder import LRFinder\nimport segmentation_models_pytorch as smp\nfrom pytorch_toolbelt.utils.rle import rle_encode, rle_to_string\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\nfrom torcheval.metrics import MulticlassConfusionMatrix\n\nplt.style.use('bmh')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:05:54.213323Z","iopub.execute_input":"2024-03-04T05:05:54.213644Z","iopub.status.idle":"2024-03-04T05:06:03.510187Z","shell.execute_reply.started":"2024-03-04T05:05:54.213614Z","shell.execute_reply":"2024-03-04T05:06:03.509423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot(x, ys:list, labels:list, title:str):\n    \"\"\"\n    \"\"\"\n    \n    fig, ax = plt.subplots(nrows=1, ncols=1, sharey=True, figsize=(10, 3))\n    for i,y in enumerate(ys):\n        label = labels[i]\n        plt.plot(x, y, label=label, marker='o')\n    \n    plt.legend()\n    ax.set_xlabel(\"Epoch\")\n    ax.xaxis.set_major_locator(MultipleLocator(1))\n    ax.yaxis.set_major_locator(MultipleLocator(0.1))\n    ax.yaxis.set_minor_locator(MultipleLocator(0.05))\n    ax.grid(which='major', color='black', linestyle='-')\n    ax.grid(which='minor', color='gray', linestyle='-', alpha=0.2)\n    plt.title(title)\n    \n    plt.show()\n    return ax","metadata":{"id":"w55y8-Usn_K4","execution":{"iopub.status.busy":"2024-03-04T05:06:03.512730Z","iopub.execute_input":"2024-03-04T05:06:03.513437Z","iopub.status.idle":"2024-03-04T05:06:03.521596Z","shell.execute_reply.started":"2024-03-04T05:06:03.513397Z","shell.execute_reply":"2024-03-04T05:06:03.520669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss_and_score(epochs, H):\n    \"\"\"\n    \"\"\"\n    \n    x = [i for i in range(1, epochs+1)]\n    \n    # Loss graph\n    y = [H[\"train_loss\"], H[\"valid_loss\"]]\n    labels = [\"Train loss\", \"Validation Loss\"]\n    ax = plot(x, y, labels, \"Loss\")\n    ax.yaxis.set_major_locator(MultipleLocator(0.2))\n    ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n\n    # Score graph\n    y = [H[\"train_score\"], H[\"valid_score\"]]\n    labels = [\"Train mean Dice\", \"Validation mean Dice\"]\n    ax = plot(x, y, labels, \"Score\")\n    ax.set_ylim([0, 1])","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:06:03.523034Z","iopub.execute_input":"2024-03-04T05:06:03.523401Z","iopub.status.idle":"2024-03-04T05:06:03.537298Z","shell.execute_reply.started":"2024-03-04T05:06:03.523367Z","shell.execute_reply":"2024-03-04T05:06:03.536505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_score_per_class(H):\n    \"\"\"\n    \"\"\"\n    \n    epochs = len(H[\"per_class_score\"])\n    \n    x = [i for i in range(1, epochs+1)]\n    ys = list(zip(*H[\"per_class_score\"]))\n\n    fig, axes = plt.subplots(nrows=config.NUM_CLASSES, ncols=1, sharey=True, sharex=True, figsize=(8, 24))\n\n    for i in range(len(ys)):\n        y = ys[i]\n        axes[i].plot(x, y, label = f\"Class {i}\", marker='o')\n        axes[i].legend(loc=2)\n        axes[i].set_ylim([0, 1])\n        axes[i].xaxis.set_tick_params(which='major', length=0)\n        axes[i].xaxis.set_major_locator(MultipleLocator(1))\n        axes[i].yaxis.set_major_locator(MultipleLocator(1))\n        axes[i].yaxis.set_major_locator(MultipleLocator(0.5))\n        axes[i].yaxis.set_minor_locator(MultipleLocator(0.1))\n        axes[i].grid(which='major', color='black', linestyle='-')\n        axes[i].grid(which='minor', color='gray', linestyle='-', alpha=0.2)\n\n    fig.supxlabel('Epoch')\n    fig.supylabel('Dice coefficient')\n\n    plt.tight_layout()\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClearCache:\n    def __enter__(self):\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        torch.cuda.empty_cache()\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:06:03.538504Z","iopub.execute_input":"2024-03-04T05:06:03.538895Z","iopub.status.idle":"2024-03-04T05:06:03.552110Z","shell.execute_reply.started":"2024-03-04T05:06:03.538863Z","shell.execute_reply":"2024-03-04T05:06:03.551339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    BATCH_SIZE = 2 if torch.cuda.is_available() else 2\n    GRADIENT_ACCUMULATION_STEPS = 2\n    EPOCHS = 10\n    NUM_CLASSES = 12\n    \n    # Parameters from: https://github.com/ultralytics/yolov5/blob/95ebf68f92196975e53ebc7e971d0130432ad107/data/hyps/hyp.scratch-low.yaml\n    INITIAL_LR = 0.01\n    FINAL_LR = 0.005\n    MOMENTUM = 0.937 # SGD momentum/Adam beta1, from Yolo v5\n    WEIGHT_DECAY = 0.005 # optimizer weight decay, from Yolo v5\n    \n    NUM_WORKERS = 4  # There are 4 CPUs in Kaggle\n    TRAIN_SPLIT = 0.8\n\n    # This is basically the \"background\" class\n    MASK_FILL_VALUE = 0\n\n# Detect if we have a GPU available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#torch.manual_seed(42)\n\nconfig = Config()","metadata":{"id":"DZEbZDOGn_K6","execution":{"iopub.status.busy":"2024-03-04T05:06:03.553261Z","iopub.execute_input":"2024-03-04T05:06:03.553568Z","iopub.status.idle":"2024-03-04T05:06:03.586697Z","shell.execute_reply.started":"2024-03-04T05:06:03.553544Z","shell.execute_reply":"2024-03-04T05:06:03.585690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">1. Data Exploration</font>","metadata":{"id":"pPuWlmKmn_K8"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">1.1. Dataset Class [7 Points]</font>","metadata":{"id":"CZALAH-8n_K-"}},{"cell_type":"code","source":"class SemSegDataset(Dataset):\n    \"\"\"\n    Generic Dataset class for semantic segmentation datasets.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path,\n        images_folder,\n        masks_folder,\n        image_ids,\n        train_val_test,\n        transforms=None,\n    ):\n        \"\"\"\n        Args:\n            data_path (string): Path to the dataset folder.\n            images_folder (string): Name of the folder containing the images.\n            masks_folder (string): Name of the folder containing the masks.\n            image_ids (list): List of image IDs to include in the dataset.\n            train_val_test (string): 'train', 'val', or 'test'.\n            transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version.\n        \"\"\"\n\n        self.data_path = data_path\n        self.images_folder = images_folder\n        self.masks_folder = masks_folder\n        self.image_ids = image_ids\n        self.train_val_test = train_val_test\n        self.transforms = transforms\n\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n\n        # Get image and mask paths\n        image_path = os.path.join(self.data_path, self.images_folder, f\"{image_id}.jpg\")\n        mask_path = os.path.join(self.data_path, self.masks_folder, f\"{image_id}.png\")\n\n        # Load image and mask\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n\n        if self.transforms is not None:\n            if mask is None:\n                return self.transforms(image=image)['image']\n            else:\n                transformed = self.transforms(image=image, mask=mask)\n\n            return transformed['image'], transformed['mask']\n        \n        return image, mask","metadata":{"id":"p1372fW2n_K-","execution":{"iopub.status.busy":"2024-03-04T05:06:03.588066Z","iopub.execute_input":"2024-03-04T05:06:03.588397Z","iopub.status.idle":"2024-03-04T05:06:03.599266Z","shell.execute_reply.started":"2024-03-04T05:06:03.588370Z","shell.execute_reply":"2024-03-04T05:06:03.598410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv_path = Path(DATA_PATH) / \"train.csv\"\ntest_csv_path = Path(DATA_PATH) / \"test.csv\"\n\ntrain_df = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)\n\n# train_ids, valid_ids = torch.utils.data.random_split(train_df.ImageID, [config.TRAIN_SPLIT, 1-config.TRAIN_SPLIT])\ntest_ids = test_df.ImageID.ravel().tolist()","metadata":{"id":"uCjUJGNMn_K_","execution":{"iopub.status.busy":"2024-03-04T05:06:03.600261Z","iopub.execute_input":"2024-03-04T05:06:03.600545Z","iopub.status.idle":"2024-03-04T05:06:03.631489Z","shell.execute_reply.started":"2024-03-04T05:06:03.600521Z","shell.execute_reply":"2024-03-04T05:06:03.630614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"whole_dataset = SemSegDataset(\n    DATA_PATH,\n    \"imgs/imgs\",\n    \"masks/masks\",\n    train_df.ImageID,\n    train_val_test=\"train\"\n)\n\ndef extract_and_onehot_encode_classes_from_multilabel_masks(dataset):\n    \"\"\"\n    Extract the classes available in each multilabel binary mask and one-hot encodes them.\n\n    Returns:\n        A NumPy array of one-hot encoded classes, where the dimensions are\n        (num_images, num_classes). Each value in the array represents the presence\n        (1) or absence (0) of a specific class in the corresponding image.\n    \"\"\"\n    ys = []\n    for i in tqdm(range(len(dataset))):\n        image, mask = dataset[i]\n        y = np.unique(mask).reshape(1,-1)\n        y = torch.Tensor(y).to(torch.int64)\n        y = torch.nn.functional.one_hot(y, num_classes=config.NUM_CLASSES)\n        y = y.sum(axis=1)\n        y = y.numpy()\n        ys.append(y)\n    \n    return np.concatenate(ys, axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:06:03.634773Z","iopub.execute_input":"2024-03-04T05:06:03.635069Z","iopub.status.idle":"2024-03-04T05:06:03.642155Z","shell.execute_reply.started":"2024-03-04T05:06:03.635044Z","shell.execute_reply":"2024-03-04T05:06:03.641260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.ImageID\ny = extract_and_onehot_encode_classes_from_multilabel_masks(whole_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:06:03.643326Z","iopub.execute_input":"2024-03-04T05:06:03.643758Z","iopub.status.idle":"2024-03-04T05:08:04.522624Z","shell.execute_reply.started":"2024-03-04T05:06:03.643723Z","shell.execute_reply":"2024-03-04T05:08:04.521770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=(1-config.TRAIN_SPLIT), random_state=0)\n\nfor train_index, test_index in msss.split(X, y):\n    train_ids, valid_ids = X[train_index], X[test_index]\n    # y_train, y_test = y[train_index], y[test_index]","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:08:04.523827Z","iopub.execute_input":"2024-03-04T05:08:04.524188Z","iopub.status.idle":"2024-03-04T05:08:04.616140Z","shell.execute_reply.started":"2024-03-04T05:08:04.524154Z","shell.execute_reply":"2024-03-04T05:08:04.615377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transforms = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.ShiftScaleRotate(\n        shift_limit=0,\n        rotate_limit=0,\n        scale_limit=0.1,\n        border_mode=cv2.BORDER_CONSTANT,\n        value=0,\n        mask_value=config.MASK_FILL_VALUE,\n        interpolation=cv2.INTER_CUBIC,\n        p=0.5,\n    ),\n    A.RandomBrightnessContrast(\n        brightness_limit=0.02,\n        contrast_limit=0.02,\n        p=0.5\n    ),\n    A.ElasticTransform(\n        alpha=120,\n        sigma=20,\n        value=0,\n        mask_value=config.MASK_FILL_VALUE,\n        border_mode=cv2.BORDER_CONSTANT,\n        alpha_affine=5,\n        interpolation=cv2.INTER_CUBIC,\n        p=0.5,\n    ),\n    A.GridDistortion(\n        num_steps=10,\n        distort_limit=0.03,\n        value=0,\n        mask_value=config.MASK_FILL_VALUE,\n        border_mode=cv2.BORDER_CONSTANT,\n        p=1,\n        interpolation=cv2.INTER_CUBIC,\n    ),\n    A.Normalize(),\n    ToTensorV2(),\n])\n\nvalid_transforms = A.Compose([\n    A.Normalize(),\n    ToTensorV2()\n])\n\ntest_transforms = A.Compose([\n    A.Normalize(),\n    ToTensorV2()\n])\n\n\ntrain_dataset = SemSegDataset(\n    DATA_PATH,\n    \"imgs/imgs\",\n    \"masks/masks\",\n    train_ids.tolist(),\n    train_val_test=\"train\",\n    transforms=train_transforms,\n)\n\nvalid_dataset = SemSegDataset(\n    DATA_PATH,\n    \"imgs/imgs\",\n    \"masks/masks\",\n    valid_ids.tolist(),\n    train_val_test=\"validation\",\n    transforms=valid_transforms\n)\n\ntest_dataset = SemSegDataset(\n    DATA_PATH,\n    \"imgs/imgs\",\n    \"masks/masks\",\n    test_ids,\n    train_val_test=\"test\",\n    transforms=test_transforms\n)\n\n# Reason for drop_last: https://discuss.pytorch.org/t/error-expected-more-than-1-value-per-channel-when-training/26274/5\ntrain_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, drop_last=True, num_workers=config.NUM_WORKERS)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=config.BATCH_SIZE, shuffle=False, drop_last=True, num_workers=config.NUM_WORKERS)\n#test_dataloader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, drop_last=False, num_workers=config.NUM_WORKERS)","metadata":{"id":"MhrrJHakn_LA","execution":{"iopub.status.busy":"2024-03-04T05:08:04.617684Z","iopub.execute_input":"2024-03-04T05:08:04.618037Z","iopub.status.idle":"2024-03-04T05:08:04.630981Z","shell.execute_reply.started":"2024-03-04T05:08:04.618005Z","shell.execute_reply":"2024-03-04T05:08:04.630090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">1.2. Visualize dataset [3 Points]</font>","metadata":{"id":"wthtNFwdn_LA"}},{"cell_type":"code","source":"def draw_semantic_segmentation_batch(dataset, n_samples=3):\n    \"\"\"\n    \"\"\"\n    fig, ax = plt.subplots(nrows=n_samples, ncols=2, sharey=True, figsize=(10, 3*n_samples))\n    for i in range(n_samples):\n        image, mask = dataset[i]\n\n        # CHW -> HWC\n        image = image.permute(1, 2, 0).detach().cpu().numpy()\n        ax[i][0].imshow(image)\n        ax[i][0].set_xlabel(\"Image\")\n        ax[i][0].set_xticks([])\n        ax[i][0].set_yticks([])\n\n        mask = torch.squeeze(mask)\n        mask = mask.detach().cpu().numpy()\n        \n        # Create colors for the visualization, one for each class\n#         colormap = plt.get_cmap('jet')\n#         linear_space = np.linspace(0, 1, config.NUM_CLASSES)\n#         colors = (255*colormap(linear_space)).astype(np.uint8)\n#         # Add black for the last unused filled mask vlaue\n#         colors = np.vstack([colors, [0, 0, 0, 255]])\n\n        colors = np.array([\n            [0, 0, 0], # background\n            [248, 200, 220], # person\n            [0, 255, 0], # bike\n            [107, 107, 107], # car\n            [144, 12, 63], # drone\n            [25, 25, 112], # boat\n            [255, 0, 255], # animal\n            [255, 0, 0], # obstacle\n            [255, 195, 0], # construction\n            [9, 121, 105], # vegetation\n            [200, 200, 60], # road\n            [135, 206, 235], # sky\n        ])\n        \n        rgb_labels = colors[mask]\n        \n        ax[i][1].imshow(rgb_labels)\n        ax[i][1].set_xlabel(\"Ground truth mask\")\n        ax[i][1].set_xticks([])\n        ax[i][1].set_yticks([])\n\n    plt.tight_layout()\n    plt.show()\n    plt.close(fig)","metadata":{"id":"KoSoysyon_K5","execution":{"iopub.status.busy":"2024-03-04T05:08:04.632292Z","iopub.execute_input":"2024-03-04T05:08:04.632939Z","iopub.status.idle":"2024-03-04T05:08:04.647888Z","shell.execute_reply.started":"2024-03-04T05:08:04.632906Z","shell.execute_reply":"2024-03-04T05:08:04.647111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_semantic_segmentation_batch(train_dataset, n_samples=10)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:08:04.649006Z","iopub.execute_input":"2024-03-04T05:08:04.649329Z","iopub.status.idle":"2024-03-04T05:08:14.377699Z","shell.execute_reply.started":"2024-03-04T05:08:04.649300Z","shell.execute_reply":"2024-03-04T05:08:14.376124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_semantic_segmentation_batch(valid_dataset, n_samples=10)","metadata":{"id":"mgVcxa9ln_LB","outputId":"b63b5836-e1b8-4c20-ee11-94c17687f571","execution":{"iopub.status.busy":"2024-03-04T05:08:14.379360Z","iopub.execute_input":"2024-03-04T05:08:14.379818Z","iopub.status.idle":"2024-03-04T05:08:19.141078Z","shell.execute_reply.started":"2024-03-04T05:08:14.379776Z","shell.execute_reply":"2024-03-04T05:08:19.139441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize each class","metadata":{}},{"cell_type":"code","source":"def draw_mask_overlay(color_image, mask, class_id, alpha=0.5, color=(0, 0, 255)):\n    overlay = np.copy(color_image)\n    overlay[mask==class_id] = color\n    cv2.addWeighted(overlay, alpha, color_image, 1-alpha, 0, dst=overlay)\n    return overlay","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:08:19.142497Z","iopub.execute_input":"2024-03-04T05:08:19.142812Z","iopub.status.idle":"2024-03-04T05:08:19.148132Z","shell.execute_reply.started":"2024-03-04T05:08:19.142782Z","shell.execute_reply":"2024-03-04T05:08:19.147265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, mask = whole_dataset[0]\n#image = image.permute(1, 2, 0).detach().cpu().numpy()  # CHW -> HWC\n\nfor i in range(config.NUM_CLASSES):\n    \n    overlay = draw_mask_overlay(image, mask, i, alpha=0.5, color=(255,0,0))\n\n    fig, ax = plt.subplots(nrows=1, ncols=1, sharey=True, figsize=(6, 6))\n    ax.imshow(overlay)\n    ax.set_xlabel(f\"Class {i}\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    plt.tight_layout()\n    plt.show()\n    plt.close(fig)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:08:19.149204Z","iopub.execute_input":"2024-03-04T05:08:19.149497Z","iopub.status.idle":"2024-03-04T05:08:24.626079Z","shell.execute_reply.started":"2024-03-04T05:08:19.149449Z","shell.execute_reply":"2024-03-04T05:08:24.625151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_images_per_class(dataset):\n    \"\"\"\n    \"\"\"\n    \n    d = defaultdict(int)\n    for i in tqdm(range(len(dataset))):\n        image, mask = dataset[i]\n        classes = np.unique(mask)\n        for c in classes:\n            d[c] += 1\n    \n    return d","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:08:24.627266Z","iopub.execute_input":"2024-03-04T05:08:24.627613Z","iopub.status.idle":"2024-03-04T05:08:24.633821Z","shell.execute_reply.started":"2024-03-04T05:08:24.627582Z","shell.execute_reply":"2024-03-04T05:08:24.632842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset_count = count_images_per_class(whole_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:08:24.634887Z","iopub.execute_input":"2024-03-04T05:08:24.635149Z","iopub.status.idle":"2024-03-04T05:08:24.650663Z","shell.execute_reply.started":"2024-03-04T05:08:24.635120Z","shell.execute_reply":"2024-03-04T05:08:24.649669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NOTE: the train dataset pipeline does random cropping during augmentation,\n# that's why there may be less classes in an augmented train image than in the original image\n# train_count = count_images_per_class(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:08:24.651853Z","iopub.execute_input":"2024-03-04T05:08:24.652175Z","iopub.status.idle":"2024-03-04T05:08:24.663003Z","shell.execute_reply.started":"2024-03-04T05:08:24.652144Z","shell.execute_reply":"2024-03-04T05:08:24.662174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid_count = count_images_per_class(valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:08:24.664033Z","iopub.execute_input":"2024-03-04T05:08:24.664360Z","iopub.status.idle":"2024-03-04T05:08:24.674082Z","shell.execute_reply.started":"2024-03-04T05:08:24.664330Z","shell.execute_reply":"2024-03-04T05:08:24.673224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(config.NUM_CLASSES):\n#     print(f\"Class {i:02}: total: {dataset_count[i]:04} | train: {train_count[i]:04} | valid: {valid_count[i]:04}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:08:24.675093Z","iopub.execute_input":"2024-03-04T05:08:24.675342Z","iopub.status.idle":"2024-03-04T05:08:24.683903Z","shell.execute_reply.started":"2024-03-04T05:08:24.675320Z","shell.execute_reply":"2024-03-04T05:08:24.683030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">2. Evaluation Metrics [10 Points]</font>\n\n<p>This competition is evaluated on the mean <a href='https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient'>Dice coefficient</a\n>. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by: </p>\n\n<p>$$DSC =  \\frac{2 |X \\cap Y|}{|X|+ |Y|}$$\n$$ \\small \\mathrm{where}\\ X = Predicted\\ Set\\ of\\ Pixels,\\ \\ Y = Ground\\ Truth $$ </p>\n<p>The Dice coefficient is defined to be 1 when both X and Y are empty.</p>","metadata":{"id":"ZiRkqUCRn_LC"}},{"cell_type":"code","source":"class DiceScore(torch.nn.Module):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self, num_classes, ignore_index=0):\n        super().__init__()\n        self.num_classes = num_classes\n        self.ignore_index: int = ignore_index\n        self.eps = 1e-6\n        self.metric = MulticlassConfusionMatrix(self.num_classes)\n\n    def __call__(self, pred, target):\n        \"\"\"\n        pred: NxHxW\n        target: NxCxHxW\n        \"\"\"\n        self.metric.reset()\n        \n        self.metric.update(pred.flatten(), target.flatten())\n        conf_matrix =  self.metric.compute()\n\n        true_positive = torch.diag(conf_matrix)\n        false_positive = torch.sum(conf_matrix, 0) - true_positive\n        false_negative = torch.sum(conf_matrix, 1) - true_positive\n\n        DSC = (2 * true_positive + self.eps) / (\n            2 * true_positive + false_positive + false_negative + self.eps\n        )\n        \n        return DSC","metadata":{"id":"8DM5CjNjn_LC","execution":{"iopub.status.busy":"2024-03-04T05:08:24.684861Z","iopub.execute_input":"2024-03-04T05:08:24.685097Z","iopub.status.idle":"2024-03-04T05:08:24.695824Z","shell.execute_reply.started":"2024-03-04T05:08:24.685077Z","shell.execute_reply":"2024-03-04T05:08:24.695045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">3. Model [10 Points]</font>","metadata":{"id":"kuVi09USn_LD"}},{"cell_type":"code","source":"def make_model():\n    model = models.segmentation.deeplabv3_resnet101(weights='DeepLabV3_ResNet101_Weights.DEFAULT', progress=True)\n\n    model.classifier = models.segmentation.deeplabv3.DeepLabHead(2048, config.NUM_CLASSES)\n    model.aux_classifier[4] = torch.nn.Conv2d(256, config.NUM_CLASSES, 1)\n\n    # Another option:\n    # model.classifier[4] = nn.LazyConv2d(num_classes, 1)\n    # model.aux_classifier[4] = nn.LazyConv2d(num_classes, 1)\n\n    # This is the code for DeepLabHead(in_channels, num_classes):\n    #\n    # ASPP(in_channels, [12, 24, 36]),\n    # nn.Conv2d(256, 256, 3, padding=1, bias=False),\n    # nn.BatchNorm2d(256),\n    # nn.ReLU(),\n    # nn.Conv2d(256, num_classes, 1),\n\n    # Freeze all layers\n    for param in model.parameters():\n        param.requires_grad = False\n        \n    for param in model.classifier.parameters():\n        param.requires_grad = True\n        \n    for param in model.aux_classifier.parameters():\n        param.requires_grad = True\n\n    return model","metadata":{"id":"oS09sPJ0n_LD","execution":{"iopub.status.busy":"2024-03-04T05:08:24.697276Z","iopub.execute_input":"2024-03-04T05:08:24.697583Z","iopub.status.idle":"2024-03-04T05:08:24.711001Z","shell.execute_reply.started":"2024-03-04T05:08:24.697559Z","shell.execute_reply":"2024-03-04T05:08:24.710095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = make_model().to(device)","metadata":{"id":"cSwZV-2Xn_LE","outputId":"18c12836-4462-4704-8a68-2bbb1e98929f","execution":{"iopub.status.busy":"2024-03-04T05:08:24.712127Z","iopub.execute_input":"2024-03-04T05:08:24.712405Z","iopub.status.idle":"2024-03-04T05:08:27.845566Z","shell.execute_reply.started":"2024-03-04T05:08:24.712383Z","shell.execute_reply":"2024-03-04T05:08:27.844785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">4. Train & Inference</font>\n## <font style=\"color:green\">4.1. Train [7 Points]</font>","metadata":{"id":"ky5Ijp_Qn_LE"}},{"cell_type":"code","source":"class SoftDiceLoss(torch.nn.Module):\n    \"\"\"\n        Implementation of the Soft-Dice Loss function.\n\n        Arguments:\n            num_classes (int): number of classes.\n            eps (float): value of the floating point epsilon.\n    \"\"\"\n    def __init__(self, num_classes, eps=1e-5):\n        super().__init__()\n        self.num_classes = num_classes\n        self.eps = eps\n\n    def forward(self, preds, targets):\n        \"\"\"\n            Compute Soft-Dice Loss.\n\n            Arguments:\n                preds (torch.FloatTensor):\n                    tensor of predicted labels. The shape of the tensor is (B, num_classes, H, W).\n                targets (torch.LongTensor):\n                    tensor of ground-truth labels. The shape of the tensor is (B, H, W).\n            Returns:\n                mean_loss (float32): mean loss by class  value.\n        \"\"\"\n\n        loss = 0\n        for cls in range(self.num_classes):\n\n            # get ground truth for the current class\n            target = (targets == cls).float()\n\n            # get prediction for the current class\n            pred = preds[:, cls]\n\n            # calculate intersection\n            intersection = (pred * target).sum()\n\n            # compute dice coefficient\n            dice = (2 * intersection + self.eps) / (pred.sum() + target.sum() + self.eps)\n            \n            # compute negative logarithm from the obtained dice coefficient\n            loss = loss - dice.log()\n\n        # get mean loss by class value\n        loss = loss / self.num_classes\n\n        return loss","metadata":{"id":"PBVConPtn_LE","execution":{"iopub.status.busy":"2024-03-04T05:08:27.846722Z","iopub.execute_input":"2024-03-04T05:08:27.846998Z","iopub.status.idle":"2024-03-04T05:08:27.855340Z","shell.execute_reply.started":"2024-03-04T05:08:27.846974Z","shell.execute_reply":"2024-03-04T05:08:27.854501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CombinedCrossEntropySoftDice(torch.nn.Module):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self, loss_fn1, loss_fn2, weight1=0.5, weight2=0.5):\n        super().__init__()\n        self.loss_fn1 = loss_fn1\n        self.loss_fn2 = loss_fn2\n        self.weight1 = weight1\n        self.weight2 = weight2\n\n    def forward(self, preds_logits, targets):\n        \n        if isinstance(preds_logits, dict):\n            preds_logits = preds_logits['out']\n\n        preds_probs = preds_logits.softmax(dim=1)\n        \n        loss1 = self.loss_fn1(preds_probs, targets)\n        loss2 = self.loss_fn2(preds_logits, targets)\n\n        combined_loss = self.weight1*loss1 + self.weight2*loss2\n\n        return combined_loss","metadata":{"id":"q0Icq8EGn_LF","execution":{"iopub.status.busy":"2024-03-04T05:08:27.862660Z","iopub.execute_input":"2024-03-04T05:08:27.863192Z","iopub.status.idle":"2024-03-04T05:08:27.869849Z","shell.execute_reply.started":"2024-03-04T05:08:27.863167Z","shell.execute_reply":"2024-03-04T05:08:27.869032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smart_optimizer(model, name=\"Adam\", lr=0.001, momentum=0.9, decay=1e-5):\n    \"\"\"\n    This implements weight decay.\n    From: https://github.com/ultralytics/yolov5/blob/master/utils/torch_utils.py#L330\n    \"\"\"\n    \n    # YOLOv5 3-param group optimizer: 0) weights with decay, 1) weights no decay, 2) biases no decay\n    g = [], [], []  # optimizer parameter groups\n    bn = tuple(v for k, v in torch.nn.__dict__.items() if \"Norm\" in k)  # normalization layers, i.e. BatchNorm2d()\n    for v in model.modules():\n        for p_name, p in v.named_parameters(recurse=0):\n            if p_name == \"bias\":  # bias (no decay)\n                g[2].append(p)\n            elif p_name == \"weight\" and isinstance(v, bn):  # weight (no decay)\n                g[1].append(p)\n            else:\n                g[0].append(p)  # weight (with decay)\n\n    if name == \"Adam\":\n        optimizer = torch.optim.Adam(g[2], lr=lr, betas=(momentum, 0.999))  # adjust beta1 to momentum\n    elif name == \"AdamW\":\n        optimizer = torch.optim.AdamW(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)\n    elif name == \"RMSProp\":\n        optimizer = torch.optim.RMSprop(g[2], lr=lr, momentum=momentum)\n    elif name == \"SGD\":\n        optimizer = torch.optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)\n    else:\n        raise NotImplementedError(f\"Optimizer {name} not implemented.\")\n\n    optimizer.add_param_group({\"params\": g[0], \"weight_decay\": decay})  # add g0 with weight_decay\n    optimizer.add_param_group({\"params\": g[1], \"weight_decay\": 0.0})  # add g1 (BatchNorm2d weights)\n    print(\n        f\"{'optimizer:'} {type(optimizer).__name__}(lr={lr}) with parameter groups \"\n        f'{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias'\n    )\n    return optimizer","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:08:27.871013Z","iopub.execute_input":"2024-03-04T05:08:27.871298Z","iopub.status.idle":"2024-03-04T05:08:27.885885Z","shell.execute_reply.started":"2024-03-04T05:08:27.871265Z","shell.execute_reply":"2024-03-04T05:08:27.885080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_best_lr(lr_finder):\n    \"\"\"\n    Extract the best Learning Rate for a trained LRFinder object.\n    \"\"\"\n    \n    learning_rates = np.array(lr_finder.history[\"lr\"])\n    losses = np.array(lr_finder.history[\"loss\"])\n\n    min_grad_idx = None\n    try:\n        min_grad_idx = (np.gradient(np.array(losses))).argmin()\n    except ValueError:\n        print(\"Failed to compute the gradients, there might not be enough points.\")\n    \n    if min_grad_idx is not None:\n        best_lr = learning_rates[min_grad_idx]\n\n    return best_lr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_best_lr(model, loss_fun, end_lr=1, num_iter=200):\n    temp_optimizer = smart_optimizer(model, \"SGD\", lr=1e-7, momentum=config.MOMENTUM, decay=config.WEIGHT_DECAY)\n    lr_finder = LRFinder(model, temp_optimizer, loss_fun, device=device)\n    lr_finder.range_test(valid_dataloader, end_lr=end_lr, num_iter=num_iter, accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS)\n    lr_finder.plot()\n    lr_finder.reset()\n\n    best_lr = extract_best_lr(lr_finder)\n    return best_lr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, optimizer, loss_fun, scorer, scheduler, epochs, train_dataloader, valid_dataloader):\n    H = {\"train_loss\": [], \"train_score\": [], \"valid_loss\": [], \"valid_score\": [], 'per_class_score': []}\n\n    for e in range(0, epochs):\n\n        print(\"\\n[INFO] EPOCH: {}/{}\".format(e + 1, epochs))           \n\n        model.train()\n\n        total_epoch_train_loss = 0\n        total_epoch_valid_loss = 0\n\n        total_epoch_train_score = 0\n        total_epoch_valid_score = 0\n        \n        total_per_class_score = []\n\n        train_steps = len(train_dataloader.dataset) // config.BATCH_SIZE\n        valid_steps = len(valid_dataloader.dataset) // config.BATCH_SIZE\n\n        train_prog_bar = tqdm(train_dataloader, total=train_steps)\n        for batch_index, (x, y) in enumerate(train_prog_bar):\n\n            y = y.squeeze()\n            (x, y) = (x.to(device, dtype=torch.float32), y.to(device, dtype=torch.long))\n           \n            pred_logits = model(x)['out']\n\n            # Train loss\n            train_loss = loss_fun(pred_logits, y)\n            # For en explanation of this, see \"MLOps Engineering at Scale-Manning (2022), Ch 8.1.3\"\n            train_loss = train_loss / config.GRADIENT_ACCUMULATION_STEPS\n            total_epoch_train_loss += train_loss.item()\n            train_loss.backward()\n\n            # Train score\n            pred_probs = pred_logits.softmax(dim=1)\n            max_indices = pred_probs.argmax(dim=1)\n            train_score = scorer(max_indices, y)\n            total_epoch_train_score += float(train_score.mean())\n                        \n            # Gradient accumulation\n            if ((batch_index + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0) or (batch_index + 1 == len(train_dataloader)):\n\n                # Weights update\n                optimizer.step()\n                optimizer.zero_grad()\n\n                # Optimizer Learning Rate update\n                scheduler.step()\n\n            del pred_logits, pred_probs, max_indices, \n\n            train_prog_bar.set_description(desc=f\"Training loss: {train_loss.item():.4f} | Mean Dice score: {float(train_score.mean()):.2f}\")\n\n        # Switch off autograd for evaluation on the validation set\n        with torch.no_grad():\n            model.eval()\n\n            valid_prog_bar = tqdm(valid_dataloader, total=valid_steps)\n            for i, (x, y) in enumerate(valid_prog_bar):\n                y = y.squeeze()\n                (x, y) = (x.to(device, dtype=torch.float32), y.to(device, dtype=torch.long))\n                \n                # Validation loss\n                pred_logits = model(x)['out']\n                valid_loss = loss_fun(pred_logits, y)\n                total_epoch_valid_loss += valid_loss.item()\n                \n                # Validation score\n                pred_probs = pred_logits.softmax(dim=1)\n                max_indices = pred_probs.argmax(dim=1)\n                valid_score = scorer(max_indices, y)\n                total_epoch_valid_score += float(valid_score.mean())\n                \n                # Per-class validation score\n                total_per_class_score.append(valid_score.reshape(1,-1))\n\n                del pred_logits, pred_probs, max_indices, \n\n                valid_prog_bar.set_description(desc=f\"Validation loss: {valid_loss.item():.4f} | Mean Dice score: {float(valid_score.mean()):.2f}\")\n\n        # Average loss during the epoch\n        avg_train_loss = total_epoch_train_loss / train_steps\n        avg_valid_loss = total_epoch_valid_loss / valid_steps\n\n        # Average score during the epoch\n        avg_train_score = total_epoch_train_score / train_steps\n        avg_valid_score = total_epoch_valid_score / valid_steps\n        \n        # Average score per class\n        avg_per_class_score = np.concatenate(total_per_class_score, axis=0).sum(axis=0) / valid_steps\n\n        H[\"train_loss\"].append(avg_train_loss)\n        H[\"valid_loss\"].append(avg_valid_loss)\n        H[\"train_score\"].append(avg_train_score)\n        H[\"valid_score\"].append(avg_valid_score)\n        \n        H[\"per_class_score\"].append(avg_per_class_score)\n\n        print(\"Epoch train loss: {:.6f} | Epoch train mean Dice score: {:.4f}\".format(avg_train_loss, avg_train_score))\n        print(\"Epoch valid loss: {:.6f} | Epoch valid mean Dice score: {:.4f}\".format(avg_valid_loss, avg_valid_score))\n\n        # Serialize the model every 5 epochs\n        if (e+1)%5 == 0:\n            output_file_path = os.path.join(OUTPUT_PATH, f\"deeplabv3_model_epoch_{e+1}.pkl\")\n            torch.save(model, output_file_path)\n    \n    return H","metadata":{"id":"shB_-Fjsn_LG","outputId":"d2e5b408-b306-4778-f109-9381ad4d8ba5","execution":{"iopub.status.busy":"2024-03-04T05:10:01.329190Z","iopub.execute_input":"2024-03-04T05:10:01.329431Z","iopub.status.idle":"2024-03-04T05:10:01.351074Z","shell.execute_reply.started":"2024-03-04T05:10:01.329409Z","shell.execute_reply":"2024-03-04T05:10:01.350158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Produces a loss around 10\nloss_fun1 = SoftDiceLoss(num_classes=config.NUM_CLASSES).to(device)\n# Using reduction='mean' produces a los of around 2, using reduction=sum produces a total loss in the order of millions\n# Receives logits\nloss_fun2 = smp.losses.FocalLoss(\"multiclass\", normalized=False, reduction='mean').to(device)\n\nloss_fun = CombinedCrossEntropySoftDice(loss_fun1, loss_fun2, weight1=1, weight2=1).to(device)\n\nscorer = DiceScore(num_classes=config.NUM_CLASSES).to(device)\n\noptimizer = smart_optimizer(model, \"SGD\", lr=config.INITIAL_LR, momentum=config.MOMENTUM, decay=config.WEIGHT_DECAY)\n\n# From: https://github.com/ultralytics/yolov5/blob/95ebf68f92196975e53ebc7e971d0130432ad107/segment/train.py#L213\n# lf = lambda x: (1 - x / config.EPOCHS) * (1.0 - lrf) + lrf  # linear\n# scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:08:27.886844Z","iopub.execute_input":"2024-03-04T05:08:27.887099Z","iopub.status.idle":"2024-03-04T05:08:27.903667Z","shell.execute_reply.started":"2024-03-04T05:08:27.887077Z","shell.execute_reply":"2024-03-04T05:08:27.902791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.memory._record_memory_history(enabled='all')","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:10:01.352105Z","iopub.execute_input":"2024-03-04T05:10:01.352374Z","iopub.status.idle":"2024-03-04T05:10:01.366339Z","shell.execute_reply.started":"2024-03-04T05:10:01.352351Z","shell.execute_reply":"2024-03-04T05:10:01.365518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def oom_observer(device, alloc, device_alloc, device_free):\n#     # snapshot right after an OOM happened\n#     print('saving allocated state during OOM')\n#     snapshot = torch.cuda.memory._snapshot()\n#     pickle.dump(snapshot, open('oom_snapshot.pickle', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:10:01.367444Z","iopub.execute_input":"2024-03-04T05:10:01.367812Z","iopub.status.idle":"2024-03-04T05:10:01.377832Z","shell.execute_reply.started":"2024-03-04T05:10:01.367748Z","shell.execute_reply":"2024-03-04T05:10:01.377066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch._C._cuda_attach_out_of_memory_observer(oom_observer)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:10:01.379060Z","iopub.execute_input":"2024-03-04T05:10:01.379618Z","iopub.status.idle":"2024-03-04T05:10:01.389381Z","shell.execute_reply.started":"2024-03-04T05:10:01.379584Z","shell.execute_reply":"2024-03-04T05:10:01.388514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the Fully Connected layers","metadata":{}},{"cell_type":"code","source":"best_lr = find_best_lr(model, loss_fun, end_lr=1, num_iter=200)\nprint(f\"Best lr:\", best_lr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nlr = best_lr\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=epochs)\nwith ClearCache():\n    H = train(model, optimizer, loss_fun, scorer, scheduler, epochs, train_dataloader, valid_dataloader)\n    plot_loss_and_score(epochs, H)\n    plot_score_per_class(H)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:10:01.390583Z","iopub.execute_input":"2024-03-04T05:10:01.390919Z","iopub.status.idle":"2024-03-04T05:11:06.026925Z","shell.execute_reply.started":"2024-03-04T05:10:01.390888Z","shell.execute_reply":"2024-03-04T05:11:06.025124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the backbone layers","metadata":{}},{"cell_type":"code","source":"# Set the last backbone layers to be trainable\nfor param in model.backbone.layer4.parameters():\n    param.requires_grad = True\n\nfor param in model.backbone.layer3.parameters():\n    param.requires_grad = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_lr = find_best_lr(model, loss_fun, end_lr=1, num_iter=200)\nprint(f\"Best lr:\", best_lr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 10\nlr = best_lr\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=epochs)\nwith ClearCache():\n    H = train(model, optimizer, loss_fun, scorer, scheduler, epochs, train_dataloader, valid_dataloader)\n    plot_loss_and_score(epochs, H)\n    plot_score_per_class(H)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:11:06.028272Z","iopub.status.idle":"2024-03-04T05:11:06.028814Z","shell.execute_reply.started":"2024-03-04T05:11:06.028542Z","shell.execute_reply":"2024-03-04T05:11:06.028568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# torch.cuda.memory._record_memory_history(enabled=None)","metadata":{"execution":{"iopub.status.busy":"2024-03-04T05:11:06.030006Z","iopub.status.idle":"2024-03-04T05:11:06.030481Z","shell.execute_reply.started":"2024-03-04T05:11:06.030221Z","shell.execute_reply":"2024-03-04T05:11:06.030239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting","metadata":{}},{"cell_type":"markdown","source":"### Plot losses","metadata":{"id":"e89Nq1h0n_LG"}},{"cell_type":"markdown","source":"### Plot scores","metadata":{"id":"UnbU8kcRn_LG"}},{"cell_type":"markdown","source":"### Plot per-class Dice scores","metadata":{"id":"6b018Krvn_LH"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">4.2. Inference [3 Points]</font>","metadata":{"id":"i-YrAgJnn_LH"}},{"cell_type":"code","source":"model.eval()\n\nn_samples = 2\n\nimages, masks = next(iter(valid_dataloader))\nimages = images[:n_samples,...].to(device, dtype=torch.float32)\n\nwith torch.no_grad():\n    preds = model(images.float())[\"out\"].argmax(dim=1)\n\nfig, ax = plt.subplots(nrows=n_samples, ncols=3, sharey=True, figsize=(10, 10))\nfor i in range(n_samples):\n\n    image = images[i, ...]\n\n    # CHW -> HWC\n    image = image.permute(1, 2, 0).detach().cpu().numpy()\n\n    mask = masks[i, ...]\n    mask = torch.squeeze(mask)\n    mask = mask.detach().cpu().numpy()\n\n    pred = preds[i, ...].detach().cpu().numpy()\n\n    ax[i][0].imshow(image)\n    ax[i][0].set_xlabel(\"image\")\n    ax[i][0].set_xticks([])\n    ax[i][0].set_yticks([])\n\n    ax[i][1].imshow(mask)\n    ax[i][1].set_xlabel(\"ground-truth mask\")\n    ax[i][1].set_xticks([])\n    ax[i][1].set_yticks([])\n\n    ax[i][2].imshow(pred)\n    ax[i][2].set_xlabel(\"Prediction\")\n    ax[i][2].set_xticks([])\n    ax[i][2].set_yticks([])\n\nplt.tight_layout()\nplt.gcf().canvas.draw()\nplt.show()\nplt.close(fig)","metadata":{"id":"FXMT5o0jn_LJ","execution":{"iopub.status.busy":"2024-03-04T05:11:06.035045Z","iopub.status.idle":"2024-03-04T05:11:06.036813Z","shell.execute_reply.started":"2024-03-04T05:11:06.036537Z","shell.execute_reply":"2024-03-04T05:11:06.036560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">5. Prepare Submission CSV [10 Points]</font>\n\nFormat:\n```\nImageID,EncodedPixels\n01_0,1 1 5 1\n01_1,2 3 8 1\n02_0,1 1\n02_1,3 1\n03_0,1 1\n03_1,4 5\netc.\n```","metadata":{"id":"9YvfbqnYn_LJ"}},{"cell_type":"code","source":"output_lines = [\"ImageID,EncodedPixels\"]\n\nfor image_id in tqdm(test_ids):\n    image_path = os.path.join(DATA_PATH, \"imgs/imgs\", f\"{image_id}.jpg\")\n\n    # Load image and mask\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    transformed = test_transforms(image=image)\n    transformed = transformed['image'].to(device, dtype=torch.float32)\n    transformed = transformed.unsqueeze(0)\n    \n    with torch.no_grad():\n        pred_mask = model(transformed)['out'].argmax(dim=1)\n        pred_mask = pred_mask.detach().cpu().numpy()\n\n    for class_id in range(config.NUM_CLASSES):\n        class_mask = (pred_mask == class_id)\n        class_image = np.zeros_like(pred_mask)\n        class_image[class_mask] = pred_mask[class_mask]\n        class_image[class_image > 0] = 1\n\n        pred_rle = rle_to_string(rle_encode(class_image))\n\n        output_line = f\"{image_id}_{class_id}, {pred_rle}\"\n        output_lines.append(output_line)\n\nwith open('submission.csv', \"w\") as f:\n    out = \"\\n\".join(line.strip() for line in output_lines)\n    f.write(out)","metadata":{"id":"QxheU3RPn_LJ","execution":{"iopub.status.busy":"2024-03-04T05:11:06.039566Z","iopub.status.idle":"2024-03-04T05:11:06.039971Z","shell.execute_reply.started":"2024-03-04T05:11:06.039761Z","shell.execute_reply":"2024-03-04T05:11:06.039777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"/kaggle/working/submission.csv\")","metadata":{"id":"T2l6S4YRn_LK","execution":{"iopub.status.busy":"2024-03-04T05:11:06.040941Z","iopub.status.idle":"2024-03-04T05:11:06.041247Z","shell.execute_reply.started":"2024-03-04T05:11:06.041097Z","shell.execute_reply":"2024-03-04T05:11:06.041109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">6. Kaggle Profile Link [50 Points]</font>\n\nShare your Kaggle profile link here with us so that we can give points for the competition score.\n\nYou should have a minimum IoU of `0.60` on the test data to get all points. If the IoU is less than `0.55`, you will not get any points for the section.\n\n**You must have to submit `submission.csv` (prediction for images in `test.csv`) in `Submit Predictions` tab in Kaggle to get any evaluation in this section.**","metadata":{"id":"4vBqfZTMn_LK"}}]}