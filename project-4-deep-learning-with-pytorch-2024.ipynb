{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66644,"databundleVersionId":7555250,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3e3c412943bb4adfb3872b95d6d96e46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d25b95b9c534fa3a6896f72cb0f1d16","IPY_MODEL_7104c29b6e514827be16bc3d13cd762d","IPY_MODEL_c31d0cd577d8435780c13748e829a1fe"],"layout":"IPY_MODEL_4a4e47f987d0430fa083aea6ddd7aaaf"}},"6d25b95b9c534fa3a6896f72cb0f1d16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbba94818cae4ddd9625ec934cfe41c9","placeholder":"​","style":"IPY_MODEL_64a7bc80adcd4e78923898e2379aef85","value":"100%"}},"7104c29b6e514827be16bc3d13cd762d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfecc21e178b4337aec3e38f0cea4f67","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c49288be2a91452f8dcb880691626d38","value":100}},"c31d0cd577d8435780c13748e829a1fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c834ac8670a74300893369471b077df2","placeholder":"​","style":"IPY_MODEL_2d1c4e9edd25498d99d9184f3e66eca7","value":" 100/100 [05:08&lt;00:00,  3.05s/it]"}},"4a4e47f987d0430fa083aea6ddd7aaaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbba94818cae4ddd9625ec934cfe41c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64a7bc80adcd4e78923898e2379aef85":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfecc21e178b4337aec3e38f0cea4f67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c49288be2a91452f8dcb880691626d38":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c834ac8670a74300893369471b077df2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d1c4e9edd25498d99d9184f3e66eca7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## <font style=\"color:blue\">Project 4: Kaggle Competition - Semantic Segmentation</font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h6>Sr. no.</h6></td> <td><h6>Section</h6></td> <td><h6>Points</h6></td> </tr>\n        <tr><td><h6>1</h6></td> <td><h6>1.1. Dataset Class</h6></td> <td><h6>7</h6></td> </tr>\n        <tr><td><h6>2</h6></td> <td><h6>1.2. Visualize dataset</h6></td> <td><h6>3</h6></td> </tr>\n        <tr><td><h6>3</h6></td> <td><h6>2. Evaluation Metrics</h6></td> <td><h6>10</h6></td> </tr>\n        <tr><td><h6>4</h6></td> <td><h6>3. Model</h6></td> <td><h6>10</h6></td> </tr>\n        <tr><td><h6>5</h6></td> <td><h6>4.1. Train</h6></td> <td><h6>7</h6></td> </tr>\n        <tr><td><h6>6</h6></td> <td><h6>4.2. Inference</h6></td> <td><h6>3</h6></td> </tr>\n        <tr><td><h6>7</h6></td> <td><h6>5. Prepare Submission CSV</h6></td><td><h6>10</h6></td> </tr>\n        <tr><td><h6>8</h6></td> <td><h6>6. Kaggle Profile Link</h6></td> <td><h6>50</h6></td> </tr>\n    </table>\n</div>\n\n---\n\n<h2>Dataset Description </h2>\n<p>The dataset consists of 3,269 images in 12 classes (including background). All images were taken from drones in a variety of scales. Samples are shown below:\n<img src=\"https://github.com/ishann/aeroscapes/blob/master/assets/data_montage.png?raw=true\" width=\"800\" height=\"800\">\n<p>The data was splitted into public train set and private test set which is used for evaluation of submissions.","metadata":{"id":"ibzW-5Jfn_K0"}},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/opencv-pytorch-segmentation-project/\"\nOUTPUT_PATH = \"/kaggle/working/\"","metadata":{"id":"TszH6eB_n_K1","execution":{"iopub.status.busy":"2024-02-20T02:33:35.755975Z","iopub.execute_input":"2024-02-20T02:33:35.756270Z","iopub.status.idle":"2024-02-20T02:33:35.765630Z","shell.execute_reply.started":"2024-02-20T02:33:35.756244Z","shell.execute_reply":"2024-02-20T02:33:35.764770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --quiet albumentations\n!pip install --quiet torch-lr-finder\n!pip install --quiet segmentation-models-pytorch\n!pip install --quiet pytorch_toolbelt\n!pip install --quiet iterative-stratification\n!pip install --quiet torcheval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"QChkUuTNn_K3","outputId":"8e8704c3-1d1e-4d30-db9f-38fdfd5a4e2f","execution":{"iopub.status.busy":"2024-02-20T02:33:35.782534Z","iopub.execute_input":"2024-02-20T02:33:35.782798Z","iopub.status.idle":"2024-02-20T02:34:56.037447Z","shell.execute_reply.started":"2024-02-20T02:33:35.782775Z","shell.execute_reply":"2024-02-20T02:34:56.036182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard Library imports\nimport time\nfrom pathlib import Path\nimport os\nfrom collections import defaultdict\n\n# External imports\nimport cv2\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MultipleLocator\nimport torchvision.transforms.functional as F\nimport torchvision.transforms as T\nfrom torchvision import models\nfrom torchvision.io import read_image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import lr_scheduler\nfrom tqdm.autonotebook import tqdm\nfrom albumentations.pytorch import ToTensorV2\nimport albumentations as A\nfrom torch_lr_finder import LRFinder\nimport segmentation_models_pytorch as smp\nfrom pytorch_toolbelt.utils.rle import rle_encode, rle_to_string\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\nfrom torcheval.metrics import MulticlassConfusionMatrix\n\nplt.style.use('bmh')","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:34:56.039538Z","iopub.execute_input":"2024-02-20T02:34:56.039860Z","iopub.status.idle":"2024-02-20T02:35:02.549112Z","shell.execute_reply.started":"2024-02-20T02:34:56.039832Z","shell.execute_reply":"2024-02-20T02:35:02.548104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot(x, ys:list, labels:list):\n    \"\"\"\n    \"\"\"\n    \n    fig, ax = plt.subplots(nrows=1, ncols=1, sharey=True, figsize=(10, 3))\n    for i,y in enumerate(ys):\n        label = labels[i]\n        plt.plot(x, y, label=label)\n    plt.legend()\n    ax.set_xlabel(\"Epoch\")\n    ax.xaxis.set_major_locator(MultipleLocator(1))\n    ax.yaxis.set_major_locator(MultipleLocator(0.1))\n    ax.yaxis.set_minor_locator(MultipleLocator(0.05))\n    ax.grid(which='major', color='black', linestyle='-')\n    ax.grid(which='minor', color='gray', linestyle='-', alpha=0.2)   \n    plt.show()","metadata":{"id":"w55y8-Usn_K4","execution":{"iopub.status.busy":"2024-02-20T02:35:02.550764Z","iopub.execute_input":"2024-02-20T02:35:02.551386Z","iopub.status.idle":"2024-02-20T02:35:02.560454Z","shell.execute_reply.started":"2024-02-20T02:35:02.551348Z","shell.execute_reply":"2024-02-20T02:35:02.559556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    BATCH_SIZE = 2 if torch.cuda.is_available() else 2\n    GRADIENT_ACCUMULATION_STEPS = 16\n    EPOCHS = 5\n    NUM_CLASSES = 12\n    CROP_WIDTH = 512\n    CROP_HEIGHT = 512\n    \n    # Parameters from: https://github.com/ultralytics/yolov5/blob/95ebf68f92196975e53ebc7e971d0130432ad107/data/hyps/hyp.scratch-low.yaml\n    INITIAL_LR = 0.01\n    FINAL_LR = 0.01\n    MOMENTUM = 0.937 # SGD momentum/Adam beta1, from Yolo v5\n    WEIGHT_DECAY = 0.0005 # optimizer weight decay, from Yolo v5\n    \n    NUM_WORKERS = 4  # There are 4 CPUs in Kaggle\n    TRAIN_SPLIT = 0.8\n\n    # This is basically the \"background\" class\n    MASK_FILL_VALUE = 0\n\n# Detect if we have a GPU available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#torch.manual_seed(42)\n\nconfig = Config()","metadata":{"id":"DZEbZDOGn_K6","execution":{"iopub.status.busy":"2024-02-20T02:35:02.563204Z","iopub.execute_input":"2024-02-20T02:35:02.563551Z","iopub.status.idle":"2024-02-20T02:35:02.608639Z","shell.execute_reply.started":"2024-02-20T02:35:02.563514Z","shell.execute_reply":"2024-02-20T02:35:02.607610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">1. Data Exploration</font>","metadata":{"id":"pPuWlmKmn_K8"}},{"cell_type":"markdown","source":"## <font style=\"color:green\">1.1. Dataset Class [7 Points]</font>","metadata":{"id":"CZALAH-8n_K-"}},{"cell_type":"code","source":"class SemSegDataset(Dataset):\n    \"\"\"\n    Generic Dataset class for semantic segmentation datasets.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_path,\n        images_folder,\n        masks_folder,\n        image_ids,\n        train_val_test,\n        transforms=None,\n    ):\n        \"\"\"\n        Args:\n            data_path (string): Path to the dataset folder.\n            images_folder (string): Name of the folder containing the images.\n            masks_folder (string): Name of the folder containing the masks.\n            image_ids (list): List of image IDs to include in the dataset.\n            train_val_test (string): 'train', 'val', or 'test'.\n            transforms (callable, optional): A function/transform that takes in a sample and returns a transformed version.\n        \"\"\"\n\n        self.data_path = data_path\n        self.images_folder = images_folder\n        self.masks_folder = masks_folder\n        self.image_ids = image_ids\n        self.train_val_test = train_val_test\n        self.transforms = transforms\n\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n\n        # Get image and mask paths\n        image_path = os.path.join(self.data_path, self.images_folder, f\"{image_id}.jpg\")\n        mask_path = os.path.join(self.data_path, self.masks_folder, f\"{image_id}.png\")\n\n        # Load image and mask\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n\n        if self.transforms is not None:\n            if mask is None:\n                return self.transforms(image=image)['image']\n            else:\n                transformed = self.transforms(image=image, mask=mask)\n\n            return transformed['image'], transformed['mask']\n        \n        return image, mask","metadata":{"id":"p1372fW2n_K-","execution":{"iopub.status.busy":"2024-02-20T02:35:02.610104Z","iopub.execute_input":"2024-02-20T02:35:02.610795Z","iopub.status.idle":"2024-02-20T02:35:02.624422Z","shell.execute_reply.started":"2024-02-20T02:35:02.610756Z","shell.execute_reply":"2024-02-20T02:35:02.623436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv_path = Path(DATA_PATH) / \"train.csv\"\ntest_csv_path = Path(DATA_PATH) / \"test.csv\"\n\ntrain_df = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)\n\n# train_ids, valid_ids = torch.utils.data.random_split(train_df.ImageID, [config.TRAIN_SPLIT, 1-config.TRAIN_SPLIT])\ntest_ids = test_df.ImageID.ravel().tolist()","metadata":{"id":"uCjUJGNMn_K_","execution":{"iopub.status.busy":"2024-02-20T02:35:02.625578Z","iopub.execute_input":"2024-02-20T02:35:02.625931Z","iopub.status.idle":"2024-02-20T02:35:02.658752Z","shell.execute_reply.started":"2024-02-20T02:35:02.625902Z","shell.execute_reply":"2024-02-20T02:35:02.657897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"whole_dataset = SemSegDataset(\n    DATA_PATH,\n    \"imgs/imgs\",\n    \"masks/masks\",\n    train_df.ImageID,\n    train_val_test=\"train\"\n)\n\ndef extract_and_onehot_encode_classes_from_multilabel_masks(dataset):\n    \"\"\"\n    Extract the classes available in each multilabel binary mask and one-hot encodes them.\n\n    Returns:\n        A NumPy array of one-hot encoded classes, where the dimensions are\n        (num_images, num_classes). Each value in the array represents the presence\n        (1) or absence (0) of a specific class in the corresponding image.\n    \"\"\"\n    ys = []\n    for i in tqdm(range(len(dataset))):\n        image, mask = dataset[i]\n        y = np.unique(mask).reshape(1,-1)\n        y = torch.Tensor(y).to(torch.int64)\n        y = torch.nn.functional.one_hot(y, num_classes=config.NUM_CLASSES)\n        y = y.sum(axis=1)\n        y = y.numpy()\n        ys.append(y)\n    \n    return np.concatenate(ys, axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:35:02.660289Z","iopub.execute_input":"2024-02-20T02:35:02.660658Z","iopub.status.idle":"2024-02-20T02:35:02.668285Z","shell.execute_reply.started":"2024-02-20T02:35:02.660627Z","shell.execute_reply":"2024-02-20T02:35:02.667326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.ImageID\ny = extract_and_onehot_encode_classes_from_multilabel_masks(whole_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:35:02.669801Z","iopub.execute_input":"2024-02-20T02:35:02.670127Z","iopub.status.idle":"2024-02-20T02:36:44.586425Z","shell.execute_reply.started":"2024-02-20T02:35:02.670096Z","shell.execute_reply":"2024-02-20T02:36:44.585583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=(1-config.TRAIN_SPLIT), random_state=0)\n\nfor train_index, test_index in msss.split(X, y):\n    train_ids, valid_ids = X[train_index], X[test_index]\n    # y_train, y_test = y[train_index], y[test_index]","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:36:44.587620Z","iopub.execute_input":"2024-02-20T02:36:44.587889Z","iopub.status.idle":"2024-02-20T02:36:44.672732Z","shell.execute_reply.started":"2024-02-20T02:36:44.587866Z","shell.execute_reply":"2024-02-20T02:36:44.671628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transforms = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.ShiftScaleRotate(\n        shift_limit=0,\n        rotate_limit=0,\n        scale_limit=0.1,\n        border_mode=cv2.BORDER_CONSTANT,\n        value=0,\n        mask_value=config.MASK_FILL_VALUE,\n        interpolation=cv2.INTER_CUBIC,\n        p=0.5,\n    ),\n    A.RandomBrightnessContrast(\n        brightness_limit=0.02,\n        contrast_limit=0.02,\n        p=0.5\n    ),\n    A.ElasticTransform(\n        alpha=120,\n        sigma=20,\n        value=0,\n        mask_value=config.MASK_FILL_VALUE,\n        border_mode=cv2.BORDER_CONSTANT,\n        alpha_affine=5,\n        interpolation=cv2.INTER_CUBIC,\n        p=0.5,\n    ),\n    A.GridDistortion(\n        num_steps=10,\n        distort_limit=0.03,\n        value=0,\n        mask_value=config.MASK_FILL_VALUE,\n        border_mode=cv2.BORDER_CONSTANT,\n        p=1,\n        interpolation=cv2.INTER_CUBIC,\n    ),\n    A.Normalize(),\n    ToTensorV2(),\n])\n\nvalid_transforms = A.Compose([\n    A.Normalize(),\n    ToTensorV2()\n])\n\ntest_transforms = A.Compose([\n    A.Normalize(),\n    ToTensorV2()\n])\n\n\ntrain_dataset = SemSegDataset(\n    DATA_PATH,\n    \"imgs/imgs\",\n    \"masks/masks\",\n    train_ids.tolist(),\n    train_val_test=\"train\",\n    transforms=train_transforms,\n)\n\nvalid_dataset = SemSegDataset(\n    DATA_PATH,\n    \"imgs/imgs\",\n    \"masks/masks\",\n    valid_ids.tolist(),\n    train_val_test=\"validation\",\n    transforms=valid_transforms\n)\n\ntest_dataset = SemSegDataset(\n    DATA_PATH,\n    \"imgs/imgs\",\n    \"masks/masks\",\n    test_ids,\n    train_val_test=\"test\",\n    transforms=test_transforms\n)\n\n# Reason for drop_last: https://discuss.pytorch.org/t/error-expected-more-than-1-value-per-channel-when-training/26274/5\ntrain_dataloader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, drop_last=True, num_workers=config.NUM_WORKERS)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=config.BATCH_SIZE, shuffle=False, drop_last=True, num_workers=config.NUM_WORKERS)\n#test_dataloader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, drop_last=False, num_workers=config.NUM_WORKERS)","metadata":{"id":"MhrrJHakn_LA","execution":{"iopub.status.busy":"2024-02-20T02:36:44.676780Z","iopub.execute_input":"2024-02-20T02:36:44.677372Z","iopub.status.idle":"2024-02-20T02:36:44.691568Z","shell.execute_reply.started":"2024-02-20T02:36:44.677340Z","shell.execute_reply":"2024-02-20T02:36:44.690600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">1.2. Visualize dataset [3 Points]</font>","metadata":{"id":"wthtNFwdn_LA"}},{"cell_type":"code","source":"def draw_semantic_segmentation_batch(dataset, n_samples=3):\n    \"\"\"\n    \"\"\"\n    fig, ax = plt.subplots(nrows=n_samples, ncols=2, sharey=True, figsize=(10, 3*n_samples))\n    for i in range(n_samples):\n        image, mask = dataset[i]\n\n        # CHW -> HWC\n        image = image.permute(1, 2, 0).detach().cpu().numpy()\n        ax[i][0].imshow(image)\n        ax[i][0].set_xlabel(\"Image\")\n        ax[i][0].set_xticks([])\n        ax[i][0].set_yticks([])\n\n        mask = torch.squeeze(mask)\n        mask = mask.detach().cpu().numpy()\n        \n        # Create colors for the visualization, one for each class\n#         colormap = plt.get_cmap('jet')\n#         linear_space = np.linspace(0, 1, config.NUM_CLASSES)\n#         colors = (255*colormap(linear_space)).astype(np.uint8)\n#         # Add black for the last unused filled mask vlaue\n#         colors = np.vstack([colors, [0, 0, 0, 255]])\n\n        colors = np.array([\n            [0, 0, 0], # background\n            [248, 200, 220], # person\n            [0, 255, 0], # bike\n            [107, 107, 107], # car\n            [144, 12, 63], # drone\n            [25, 25, 112], # boat\n            [255, 0, 255], # animal\n            [255, 0, 0], # obstacle\n            [255, 195, 0], # construction\n            [9, 121, 105], # vegetation\n            [200, 200, 60], # road\n            [135, 206, 235], # sky\n        ])\n        \n        rgb_labels = colors[mask]\n        \n        ax[i][1].imshow(rgb_labels)\n        ax[i][1].set_xlabel(\"Ground truth mask\")\n        ax[i][1].set_xticks([])\n        ax[i][1].set_yticks([])\n\n    plt.tight_layout()\n    plt.show()\n    plt.close(fig)","metadata":{"id":"KoSoysyon_K5","execution":{"iopub.status.busy":"2024-02-20T02:36:44.692659Z","iopub.execute_input":"2024-02-20T02:36:44.692932Z","iopub.status.idle":"2024-02-20T02:36:44.707367Z","shell.execute_reply.started":"2024-02-20T02:36:44.692907Z","shell.execute_reply":"2024-02-20T02:36:44.706563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_semantic_segmentation_batch(train_dataset, n_samples=10)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:36:44.708394Z","iopub.execute_input":"2024-02-20T02:36:44.708773Z","iopub.status.idle":"2024-02-20T02:36:53.861850Z","shell.execute_reply.started":"2024-02-20T02:36:44.708748Z","shell.execute_reply":"2024-02-20T02:36:53.860508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_semantic_segmentation_batch(valid_dataset, n_samples=10)","metadata":{"id":"mgVcxa9ln_LB","outputId":"b63b5836-e1b8-4c20-ee11-94c17687f571","execution":{"iopub.status.busy":"2024-02-20T02:36:53.863220Z","iopub.execute_input":"2024-02-20T02:36:53.863553Z","iopub.status.idle":"2024-02-20T02:36:58.615112Z","shell.execute_reply.started":"2024-02-20T02:36:53.863525Z","shell.execute_reply":"2024-02-20T02:36:58.613668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize each class","metadata":{}},{"cell_type":"code","source":"def draw_mask_overlay(color_image, mask, class_id, alpha=0.5, color=(0, 0, 255)):\n    overlay = np.copy(color_image)\n    overlay[mask==class_id] = color\n    cv2.addWeighted(overlay, alpha, color_image, 1-alpha, 0, dst=overlay)\n    return overlay","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:36:58.617047Z","iopub.execute_input":"2024-02-20T02:36:58.617391Z","iopub.status.idle":"2024-02-20T02:36:58.622800Z","shell.execute_reply.started":"2024-02-20T02:36:58.617351Z","shell.execute_reply":"2024-02-20T02:36:58.621690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image, mask = whole_dataset[0]\n#image = image.permute(1, 2, 0).detach().cpu().numpy()  # CHW -> HWC\n\nfor i in range(config.NUM_CLASSES):\n    \n    overlay = draw_mask_overlay(image, mask, i, alpha=0.5, color=(255,0,0))\n\n    fig, ax = plt.subplots(nrows=1, ncols=1, sharey=True, figsize=(6, 6))\n    ax.imshow(overlay)\n    ax.set_xlabel(f\"Class {i}\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    plt.tight_layout()\n    plt.show()\n    plt.close(fig)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:36:58.624055Z","iopub.execute_input":"2024-02-20T02:36:58.624701Z","iopub.status.idle":"2024-02-20T02:37:03.879793Z","shell.execute_reply.started":"2024-02-20T02:36:58.624669Z","shell.execute_reply":"2024-02-20T02:37:03.878885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_images_per_class(dataset):\n    \"\"\"\n    \"\"\"\n    \n    d = defaultdict(int)\n    for i in tqdm(range(len(dataset))):\n        image, mask = dataset[i]\n        classes = np.unique(mask)\n        for c in classes:\n            d[c] += 1\n    \n    return d","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:37:03.880889Z","iopub.execute_input":"2024-02-20T02:37:03.881177Z","iopub.status.idle":"2024-02-20T02:37:03.886317Z","shell.execute_reply.started":"2024-02-20T02:37:03.881151Z","shell.execute_reply":"2024-02-20T02:37:03.885498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dataset_count = count_images_per_class(whole_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:37:03.887455Z","iopub.execute_input":"2024-02-20T02:37:03.887757Z","iopub.status.idle":"2024-02-20T02:37:03.902466Z","shell.execute_reply.started":"2024-02-20T02:37:03.887733Z","shell.execute_reply":"2024-02-20T02:37:03.901666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NOTE: the train dataset pipeline does random cropping during augmentation,\n# that's why there may be less classes in an augmented train image than in the original image\n# train_count = count_images_per_class(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:37:03.903499Z","iopub.execute_input":"2024-02-20T02:37:03.903762Z","iopub.status.idle":"2024-02-20T02:37:03.914315Z","shell.execute_reply.started":"2024-02-20T02:37:03.903739Z","shell.execute_reply":"2024-02-20T02:37:03.913624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid_count = count_images_per_class(valid_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:37:03.915284Z","iopub.execute_input":"2024-02-20T02:37:03.915565Z","iopub.status.idle":"2024-02-20T02:37:03.924374Z","shell.execute_reply.started":"2024-02-20T02:37:03.915541Z","shell.execute_reply":"2024-02-20T02:37:03.923443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in range(config.NUM_CLASSES):\n#     print(f\"Class {i:02}: total: {dataset_count[i]:04} | train: {train_count[i]:04} | valid: {valid_count[i]:04}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:37:03.925484Z","iopub.execute_input":"2024-02-20T02:37:03.925773Z","iopub.status.idle":"2024-02-20T02:37:03.935127Z","shell.execute_reply.started":"2024-02-20T02:37:03.925749Z","shell.execute_reply":"2024-02-20T02:37:03.934422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">2. Evaluation Metrics [10 Points]</font>\n\n<p>This competition is evaluated on the mean <a href='https://en.wikipedia.org/wiki/Sørensen–Dice_coefficient'>Dice coefficient</a\n>. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by: </p>\n\n<p>$$DSC =  \\frac{2 |X \\cap Y|}{|X|+ |Y|}$$\n$$ \\small \\mathrm{where}\\ X = Predicted\\ Set\\ of\\ Pixels,\\ \\ Y = Ground\\ Truth $$ </p>\n<p>The Dice coefficient is defined to be 1 when both X and Y are empty.</p>","metadata":{"id":"ZiRkqUCRn_LC"}},{"cell_type":"code","source":"class DiceScore(torch.nn.Module):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self, num_classes, ignore_index=0):\n        super().__init__()\n        self.num_classes = num_classes\n        self.ignore_index: int = ignore_index\n        self.eps = 1e-6\n        self.metric = MulticlassConfusionMatrix(self.num_classes)\n\n    def __call__(self, pred, target):\n        \"\"\"\n        pred: NxHxW\n        target: NxCxHxW\n        \"\"\"\n        self.metric.reset()\n        \n        self.metric.update(pred.flatten(), target.flatten())\n        conf_matrix =  self.metric.compute()\n\n        true_positive = torch.diag(conf_matrix)\n        false_positive = torch.sum(conf_matrix, 0) - true_positive\n        false_negative = torch.sum(conf_matrix, 1) - true_positive\n\n        DSC = (2 * true_positive + self.eps) / (\n            2 * true_positive + false_positive + false_negative + self.eps\n        )\n        \n        return DSC","metadata":{"id":"8DM5CjNjn_LC","execution":{"iopub.status.busy":"2024-02-20T02:37:03.936290Z","iopub.execute_input":"2024-02-20T02:37:03.936872Z","iopub.status.idle":"2024-02-20T02:37:03.946200Z","shell.execute_reply.started":"2024-02-20T02:37:03.936845Z","shell.execute_reply":"2024-02-20T02:37:03.945377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">3. Model [10 Points]</font>","metadata":{"id":"kuVi09USn_LD"}},{"cell_type":"code","source":"def make_model():\n    model = models.segmentation.deeplabv3_resnet101(weights='DeepLabV3_ResNet101_Weights.DEFAULT', progress=True)\n\n    model.classifier = models.segmentation.deeplabv3.DeepLabHead(2048, config.NUM_CLASSES)\n    model.aux_classifier[4] = torch.nn.Conv2d(256, config.NUM_CLASSES, 1)\n\n    # Another option:\n    # model.classifier[4] = nn.LazyConv2d(num_classes, 1)\n    # model.aux_classifier[4] = nn.LazyConv2d(num_classes, 1)\n\n    # This is the code for DeepLabHead(in_channels, num_classes):\n    #\n    # ASPP(in_channels, [12, 24, 36]),\n    # nn.Conv2d(256, 256, 3, padding=1, bias=False),\n    # nn.BatchNorm2d(256),\n    # nn.ReLU(),\n    # nn.Conv2d(256, num_classes, 1),\n\n    # Freeze all layers\n    for param in model.parameters():\n        param.requires_grad = False\n    \n    # Set the last backbone layer to be trainable\n    for param in model.backbone.layer4.parameters():\n        param.requires_grad = True\n        \n    for param in model.classifier.parameters():\n        param.requires_grad = True\n        \n    for param in model.aux_classifier.parameters():\n        param.requires_grad = True\n\n    return model","metadata":{"id":"oS09sPJ0n_LD","execution":{"iopub.status.busy":"2024-02-20T02:37:03.947228Z","iopub.execute_input":"2024-02-20T02:37:03.947556Z","iopub.status.idle":"2024-02-20T02:37:03.960374Z","shell.execute_reply.started":"2024-02-20T02:37:03.947532Z","shell.execute_reply":"2024-02-20T02:37:03.959559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = make_model().to(device)","metadata":{"id":"cSwZV-2Xn_LE","outputId":"18c12836-4462-4704-8a68-2bbb1e98929f","execution":{"iopub.status.busy":"2024-02-20T02:37:03.961378Z","iopub.execute_input":"2024-02-20T02:37:03.961678Z","iopub.status.idle":"2024-02-20T02:37:09.846226Z","shell.execute_reply.started":"2024-02-20T02:37:03.961653Z","shell.execute_reply":"2024-02-20T02:37:09.845269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">4. Train & Inference</font>\n## <font style=\"color:green\">4.1. Train [7 Points]</font>","metadata":{"id":"ky5Ijp_Qn_LE"}},{"cell_type":"code","source":"class SoftDiceLoss(torch.nn.Module):\n    \"\"\"\n        Implementation of the Soft-Dice Loss function.\n\n        Arguments:\n            num_classes (int): number of classes.\n            eps (float): value of the floating point epsilon.\n    \"\"\"\n    def __init__(self, num_classes, eps=1e-5):\n        super().__init__()\n        self.num_classes = num_classes\n        self.eps = eps\n\n    def forward(self, preds, targets):\n        \"\"\"\n            Compute Soft-Dice Loss.\n\n            Arguments:\n                preds (torch.FloatTensor):\n                    tensor of predicted labels. The shape of the tensor is (B, num_classes, H, W).\n                targets (torch.LongTensor):\n                    tensor of ground-truth labels. The shape of the tensor is (B, H, W).\n            Returns:\n                mean_loss (float32): mean loss by class  value.\n        \"\"\"\n\n        loss = 0\n        for cls in range(self.num_classes):\n\n            # get ground truth for the current class\n            target = (targets == cls).float()\n\n            # get prediction for the current class\n            pred = preds[:, cls]\n\n            # calculate intersection\n            intersection = (pred * target).sum()\n\n            # compute dice coefficient\n            dice = (2 * intersection + self.eps) / (pred.sum() + target.sum() + self.eps)\n            \n            # compute negative logarithm from the obtained dice coefficient\n            loss = loss - dice.log()\n\n        # get mean loss by class value\n        loss = loss / self.num_classes\n\n        return loss","metadata":{"id":"PBVConPtn_LE","execution":{"iopub.status.busy":"2024-02-20T02:37:09.847372Z","iopub.execute_input":"2024-02-20T02:37:09.847690Z","iopub.status.idle":"2024-02-20T02:37:09.856293Z","shell.execute_reply.started":"2024-02-20T02:37:09.847658Z","shell.execute_reply":"2024-02-20T02:37:09.855150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, x, y, loss_fun, metric):\n    \"\"\"\n    \"\"\"\n    \n    pred_logits = model(x)['out']\n    pred_probs = pred_logits.softmax(dim=1)\n    \n    loss = loss_fun(pred_logits, y)\n\n    max_indices = pred_probs.argmax(dim=1)\n\n    score = metric(max_indices, y)\n\n    return loss, score, max_indices","metadata":{"id":"3ljllnNgn_LE","execution":{"iopub.status.busy":"2024-02-20T02:37:09.857452Z","iopub.execute_input":"2024-02-20T02:37:09.857774Z","iopub.status.idle":"2024-02-20T02:37:09.871059Z","shell.execute_reply.started":"2024-02-20T02:37:09.857726Z","shell.execute_reply":"2024-02-20T02:37:09.870195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CombinedCrossEntropySoftDice(torch.nn.Module):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self, loss_fn1, loss_fn2, weight1=0.5, weight2=0.5):\n        super().__init__()\n        self.loss_fn1 = loss_fn1\n        self.loss_fn2 = loss_fn2\n        self.weight1 = weight1\n        self.weight2 = weight2\n\n    def forward(self, preds_logits, targets):\n        \n        if isinstance(preds_logits, dict):\n            preds_logits = preds_logits['out']\n\n        preds_probs = preds_logits.softmax(dim=1)\n        \n        loss1 = self.loss_fn1(preds_probs, targets)\n        loss2 = self.loss_fn2(preds_logits, targets)\n\n        combined_loss = self.weight1*loss1 + self.weight2*loss2\n\n        return combined_loss","metadata":{"id":"q0Icq8EGn_LF","execution":{"iopub.status.busy":"2024-02-20T02:37:09.872166Z","iopub.execute_input":"2024-02-20T02:37:09.872473Z","iopub.status.idle":"2024-02-20T02:37:09.880314Z","shell.execute_reply.started":"2024-02-20T02:37:09.872444Z","shell.execute_reply":"2024-02-20T02:37:09.879597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def smart_optimizer(model, name=\"Adam\", lr=0.001, momentum=0.9, decay=1e-5):\n    \"\"\"\n    This implements weight decay.\n    From: https://github.com/ultralytics/yolov5/blob/master/utils/torch_utils.py#L330\n    \"\"\"\n    \n    # YOLOv5 3-param group optimizer: 0) weights with decay, 1) weights no decay, 2) biases no decay\n    g = [], [], []  # optimizer parameter groups\n    bn = tuple(v for k, v in torch.nn.__dict__.items() if \"Norm\" in k)  # normalization layers, i.e. BatchNorm2d()\n    for v in model.modules():\n        for p_name, p in v.named_parameters(recurse=0):\n            if p_name == \"bias\":  # bias (no decay)\n                g[2].append(p)\n            elif p_name == \"weight\" and isinstance(v, bn):  # weight (no decay)\n                g[1].append(p)\n            else:\n                g[0].append(p)  # weight (with decay)\n\n    if name == \"Adam\":\n        optimizer = torch.optim.Adam(g[2], lr=lr, betas=(momentum, 0.999))  # adjust beta1 to momentum\n    elif name == \"AdamW\":\n        optimizer = torch.optim.AdamW(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)\n    elif name == \"RMSProp\":\n        optimizer = torch.optim.RMSprop(g[2], lr=lr, momentum=momentum)\n    elif name == \"SGD\":\n        optimizer = torch.optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)\n    else:\n        raise NotImplementedError(f\"Optimizer {name} not implemented.\")\n\n    optimizer.add_param_group({\"params\": g[0], \"weight_decay\": decay})  # add g0 with weight_decay\n    optimizer.add_param_group({\"params\": g[1], \"weight_decay\": 0.0})  # add g1 (BatchNorm2d weights)\n    print(\n        f\"{'optimizer:'} {type(optimizer).__name__}(lr={lr}) with parameter groups \"\n        f'{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias'\n    )\n    return optimizer","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:37:09.881323Z","iopub.execute_input":"2024-02-20T02:37:09.881610Z","iopub.status.idle":"2024-02-20T02:37:09.895179Z","shell.execute_reply.started":"2024-02-20T02:37:09.881587Z","shell.execute_reply":"2024-02-20T02:37:09.894281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Produces a loss around 10\nloss_fun1 = SoftDiceLoss(num_classes=config.NUM_CLASSES).to(device)\n# Using reduction='mean' produces a los of around 2, using reduction=sum produces a total loss in the order of millions\n# Receives logits\nloss_fun2 = smp.losses.FocalLoss(\"multiclass\", ignore_index=config.MASK_FILL_VALUE, normalized=False, reduction='mean').to(device)\n\nloss_fun = CombinedCrossEntropySoftDice(loss_fun1, loss_fun2, weight1=1, weight2=1).to(device)\n\nscorer = DiceScore(num_classes=config.NUM_CLASSES).to(device)\n\n# optimizer = smart_optimizer(model, \"SGD\", lr=config.INITIAL_LR, momentum=config.MOMENTUM, decay=config.WEIGHT_DECAY)\n# optimizer = torch.optim.SGD(model.parameters(), lr=config.INITIAL_LR, momentum=config.MOMENTUM, nesterov=True)\noptimizer = torch.optim.AdamW(model.parameters(), lr=config.INITIAL_LR)\n\n# From: https://github.com/ultralytics/yolov5/blob/95ebf68f92196975e53ebc7e971d0130432ad107/segment/train.py#L213\n# lf = lambda x: (1 - x / config.EPOCHS) * (1.0 - lrf) + lrf  # linear\n# scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)  # plot_lr_scheduler(optimizer, scheduler, epochs)\n\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config.FINAL_LR, steps_per_epoch=len(train_dataloader), epochs=config.EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-02-20T02:44:45.121171Z","iopub.execute_input":"2024-02-20T02:44:45.122162Z","iopub.status.idle":"2024-02-20T02:44:45.164367Z","shell.execute_reply.started":"2024-02-20T02:44:45.122123Z","shell.execute_reply":"2024-02-20T02:44:45.162959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Find best initial Learning Rate\n# temp_opt = torch.optim.SGD(model.parameters(), lr=config.INITIAL_LR, momentum=config.MOMENTUM, nesterov=True)\n# lr_finder = LRFinder(model, temp_opt, loss_fun, device=device)\n# # Using it with val_loader=valid_dataloader is super slow\n# lr_finder.range_test(train_dataloader, end_lr=1, num_iter=100)\n# lr_finder.plot()\n# lr_finder.reset()","metadata":{"id":"M_sN28hKn_LF","outputId":"0c7ae00a-70a0-4af7-b8c9-9370487c23b5","execution":{"iopub.status.busy":"2024-02-20T02:44:45.434765Z","iopub.execute_input":"2024-02-20T02:44:45.435508Z","iopub.status.idle":"2024-02-20T02:44:45.439571Z","shell.execute_reply.started":"2024-02-20T02:44:45.435475Z","shell.execute_reply":"2024-02-20T02:44:45.438693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the best Learning Rate\n# lrs = np.array(lr_finder.history[\"lr\"])\n# losses = np.array(lr_finder.history[\"loss\"])\n# \n# min_grad_idx = None\n# try:\n#     min_grad_idx = (np.gradient(np.array(losses))).argmin()\n# except ValueError:\n#     print(\"Failed to compute the gradients, there might not be enough points.\")\n# if min_grad_idx is not None:\n#     best_lr = lrs[min_grad_idx]\n# \n# print(f\"Best lr:\", best_lr)\n\n# Setup the optimizer with the new Learning Rate\n# optimizer = torch.optim.SGD(model.parameters(), lr=best_lr, momentum=0.9)","metadata":{"id":"iy7mgRw0n_LG","outputId":"d105a834-d7c3-433f-b055-10603cf57f91","execution":{"iopub.status.busy":"2024-02-20T02:44:45.594068Z","iopub.execute_input":"2024-02-20T02:44:45.594898Z","iopub.status.idle":"2024-02-20T02:44:45.599223Z","shell.execute_reply.started":"2024-02-20T02:44:45.594864Z","shell.execute_reply":"2024-02-20T02:44:45.598182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"H = {\"train_loss\": [], \"train_score\": [], \"valid_loss\": [], \"valid_score\": [], 'per_class_score': []}\n\nfor e in range(0, config.EPOCHS):\n\n    print(\"\\n[INFO] EPOCH: {}/{}\".format(e + 1, config.EPOCHS))\n\n    model.train()\n\n    total_epoch_train_loss = 0\n    total_epoch_valid_loss = 0\n\n    total_epoch_train_score = 0\n    total_epoch_valid_score = 0\n\n    train_steps = len(train_dataset) // config.BATCH_SIZE\n    valid_steps = len(valid_dataset) // config.BATCH_SIZE\n\n    train_prog_bar = tqdm(train_dataloader, total=train_steps)\n    for batch_index, (x, y) in enumerate(train_prog_bar):\n\n        y = y.squeeze()\n        (x, y) = (x.to(device, dtype=torch.float32), y.to(device, dtype=torch.long))\n\n        train_loss, train_score, pred = evaluate_model(model, x, y, loss_fun, scorer)\n        \n        # For en explanation of this, see \"MLOps Engineering at Scale-Manning (2022), Ch 8.1.3\"\n        train_loss = train_loss / config.GRADIENT_ACCUMULATION_STEPS\n\n        total_epoch_train_loss += train_loss.item()\n        train_loss.backward()\n        \n        total_epoch_train_score += np.nanmean(train_score)\n\n        # Gradient accumulation\n        if ((batch_index + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0) or (batch_index + 1 == len(train_dataloader)):\n            \n            # Weights update\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Optimizer Learning Rate update\n            scheduler.step()\n\n        train_prog_bar.set_description(desc=f\"Training loss: {train_loss.item():.4f} | Mean Dice score: {np.nanmean(train_score):.2f}\")\n\n    # Switch off autograd for evaluation on the validation set\n    with torch.no_grad():\n        model.eval()\n\n        valid_prog_bar = tqdm(valid_dataloader, total=valid_steps)\n        for i, (x, y) in enumerate(valid_prog_bar):\n            y = y.squeeze()\n            (x, y) = (x.to(device, dtype=torch.float32), y.to(device, dtype=torch.long))\n\n            valid_loss, valid_score, pred = evaluate_model(model, x, y, loss_fun, scorer)\n\n            total_epoch_valid_loss += valid_loss.item()\n            total_epoch_valid_score += np.nanmean(valid_score)\n\n            valid_prog_bar.set_description(desc=f\"Validation loss: {valid_loss.item():.4f} | Mean Dice score: {np.nanmean(valid_score):.2f}\")\n\n    avg_train_loss = total_epoch_train_loss / train_steps\n    avg_valid_loss = total_epoch_valid_loss / valid_steps\n\n    avg_train_score = total_epoch_train_score / train_steps\n    avg_valid_score = total_epoch_valid_score / valid_steps\n\n    H[\"train_loss\"].append(avg_train_loss)\n    H[\"valid_loss\"].append(avg_valid_loss)\n    H[\"train_score\"].append(avg_train_score)\n    H[\"valid_score\"].append(avg_valid_score)\n    H[\"per_class_score\"].append(valid_score)\n\n    print(\"Epoch train loss: {:.6f} | Epoch train mean Dice score: {:.4f}\".format(avg_train_loss, avg_train_score))\n    print(\"Epoch valid loss: {:.6f} | Epoch valid mean Dice score: {:.4f}\".format(avg_valid_loss, avg_valid_score))\n\n    # Serialize the model every 5 epochs\n    if (e+1)%5 == 0:\n        output_file_path = os.path.join(OUTPUT_PATH, f\"deeplabv3_model_epoch_{e+1}.pkl\")\n        torch.save(model, output_file_path)\n\n    # Update learning rate\n#     if scheduler is not None:\n#         if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n#             scheduler.step(avg_valid_loss)\n#             print(\"Bad Epochs:{}\".format(scheduler.num_bad_epochs))\n#             print(f\"Last Learning Rate = {optimizer.param_groups[0]['lr']}\")\n#         else:\n#             scheduler.step()\n#             print(f\"LR param group 0: {optimizer.param_groups[0]['lr']}\")\n#             print(f\"LR param group 1: {optimizer.param_groups[1]['lr']}\")\n#             print(f\"LR param group 2: {optimizer.param_groups[2]['lr']}\")","metadata":{"id":"shB_-Fjsn_LG","outputId":"d2e5b408-b306-4778-f109-9381ad4d8ba5","execution":{"iopub.status.busy":"2024-02-20T02:44:45.767924Z","iopub.execute_input":"2024-02-20T02:44:45.768307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting","metadata":{}},{"cell_type":"code","source":"x = [i for i in range(1, config.EPOCHS+1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot losses","metadata":{"id":"e89Nq1h0n_LG"}},{"cell_type":"code","source":"y = [H[\"train_loss\"], H[\"valid_loss\"]]\nlabels = [\"Train loss\", \"Validation Loss\"]\nplot(x, y, labels)","metadata":{"id":"rrZeUHCAn_LG","execution":{"iopub.status.busy":"2024-02-20T02:44:04.417334Z","iopub.status.idle":"2024-02-20T02:44:04.417821Z","shell.execute_reply.started":"2024-02-20T02:44:04.417583Z","shell.execute_reply":"2024-02-20T02:44:04.417606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot scores","metadata":{"id":"UnbU8kcRn_LG"}},{"cell_type":"code","source":"y = [H[\"train_score\"], H[\"valid_score\"]]\nlabels = [\"Train mean Dice\", \"Validation mean Dice\"]\nplot(x, y, labels)","metadata":{"id":"lJy7VMlhn_LG","execution":{"iopub.status.busy":"2024-02-20T02:44:04.419447Z","iopub.status.idle":"2024-02-20T02:44:04.419806Z","shell.execute_reply.started":"2024-02-20T02:44:04.419639Z","shell.execute_reply":"2024-02-20T02:44:04.419655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot per-class Dice scores","metadata":{"id":"6b018Krvn_LH"}},{"cell_type":"code","source":"ys = list(zip(*H[\"per_class_score\"]))\n\nfig, axes = plt.subplots(nrows=config.NUM_CLASSES, ncols=1, sharey=True, sharex=True, figsize=(8, 24))\n\nfor i in range(len(ys)):\n    y = ys[i]\n    axes[i].plot(x, y, label = f\"Class {i}\", marker='o')\n    axes[i].legend(loc=2)\n    axes[i].xaxis.set_tick_params(which='major', length=0)\n    axes[i].xaxis.set_major_locator(MultipleLocator(1))\n    axes[i].yaxis.set_major_locator(MultipleLocator(1))\n    axes[i].yaxis.set_major_locator(MultipleLocator(0.5))\n    axes[i].yaxis.set_minor_locator(MultipleLocator(0.1))\n    axes[i].grid(which='major', color='black', linestyle='-')\n    axes[i].grid(which='minor', color='gray', linestyle='-', alpha=0.2)\n\nfig.supxlabel('Epoch')\nfig.supylabel('Dice coefficient')\n\nplt.tight_layout()\nplt.show()","metadata":{"id":"n0OiBFMGn_LH","execution":{"iopub.status.busy":"2024-02-20T02:44:04.421202Z","iopub.status.idle":"2024-02-20T02:44:04.421597Z","shell.execute_reply.started":"2024-02-20T02:44:04.421382Z","shell.execute_reply":"2024-02-20T02:44:04.421399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font style=\"color:green\">4.2. Inference [3 Points]</font>","metadata":{"id":"i-YrAgJnn_LH"}},{"cell_type":"code","source":"model.eval()\n\nn_samples = 2\n\nimages, masks = next(iter(valid_dataloader))\nimages = images[:n_samples,...].to(device, dtype=torch.float32)\n\nwith torch.no_grad():\n    preds = model(images.float())[\"out\"].argmax(dim=1)\n\nfig, ax = plt.subplots(nrows=n_samples, ncols=3, sharey=True, figsize=(10, 10))\nfor i in range(n_samples):\n\n    image = images[i, ...]\n\n    # CHW -> HWC\n    image = image.permute(1, 2, 0).detach().cpu().numpy()\n\n    mask = masks[i, ...]\n    mask = torch.squeeze(mask)\n    mask = mask.detach().cpu().numpy()\n\n    pred = preds[i, ...].detach().cpu().numpy()\n\n    ax[i][0].imshow(image)\n    ax[i][0].set_xlabel(\"image\")\n    ax[i][0].set_xticks([])\n    ax[i][0].set_yticks([])\n\n    ax[i][1].imshow(mask)\n    ax[i][1].set_xlabel(\"ground-truth mask\")\n    ax[i][1].set_xticks([])\n    ax[i][1].set_yticks([])\n\n    ax[i][2].imshow(pred)\n    ax[i][2].set_xlabel(\"Prediction\")\n    ax[i][2].set_xticks([])\n    ax[i][2].set_yticks([])\n\nplt.tight_layout()\nplt.gcf().canvas.draw()\nplt.show()\nplt.close(fig)","metadata":{"id":"FXMT5o0jn_LJ","execution":{"iopub.status.busy":"2024-02-20T02:44:04.423079Z","iopub.status.idle":"2024-02-20T02:44:04.423450Z","shell.execute_reply.started":"2024-02-20T02:44:04.423256Z","shell.execute_reply":"2024-02-20T02:44:04.423273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">5. Prepare Submission CSV [10 Points]</font>\n\nFormat:\n```\nImageID,EncodedPixels\n01_0,1 1 5 1\n01_1,2 3 8 1\n02_0,1 1\n02_1,3 1\n03_0,1 1\n03_1,4 5\netc.\n```","metadata":{"id":"9YvfbqnYn_LJ"}},{"cell_type":"code","source":"# From:\n# https://www.kaggle.com/code/paulorzp/rle-functions-run-lenght-encode-decode\n\n# def mask2rle(img):\n#     '''\n#     img: numpy array, 1 - mask, 0 - background\n#     Returns run length as string formated\n#     '''\n#     pixels= img.T.flatten()\n#     pixels = np.concatenate([[0], pixels, [0]])\n#     runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n#     runs[1::2] -= runs[::2]\n#     return ' '.join(str(x) for x in runs)\n\n\n# def rle2mask(mask_rle, shape=(1600,256)):\n#     '''\n#     mask_rle: run-length as string formated (start length)\n#     shape: (width,height) of array to return\n#     Returns numpy array, 1 - mask, 0 - background\n\n#     '''\n#     s = mask_rle.split()\n#     starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n#     starts -= 1\n#     ends = starts + lengths\n#     img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n#     for lo, hi in zip(starts, ends):\n#         img[lo:hi] = 1\n#     return img.reshape(shape).T","metadata":{"id":"GWhxYQAgn_LJ","execution":{"iopub.status.busy":"2024-02-20T02:44:04.425100Z","iopub.status.idle":"2024-02-20T02:44:04.425442Z","shell.execute_reply.started":"2024-02-20T02:44:04.425259Z","shell.execute_reply":"2024-02-20T02:44:04.425274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_lines = [\"ImageID,EncodedPixels\"]\n\nfor image_id in tqdm(test_ids):\n    image_path = os.path.join(DATA_PATH, \"imgs/imgs\", f\"{image_id}.jpg\")\n\n    # Load image and mask\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    transformed = test_transforms(image=image)\n    transformed = transformed['image'].to(device, dtype=torch.float32)\n    transformed = transformed.unsqueeze(0)\n    \n    with torch.no_grad():\n        pred_mask = model(transformed)['out'].argmax(dim=1)\n        pred_mask = pred_mask.detach().cpu().numpy()\n\n    for class_id in range(config.NUM_CLASSES):\n        class_mask = (pred_mask == class_id)\n        class_image = np.zeros_like(pred_mask)\n        class_image[class_mask] = pred_mask[class_mask]\n        class_image[class_image > 0] = 1\n\n        pred_rle = rle_to_string(rle_encode(class_image))\n\n        output_line = f\"{image_id}_{class_id}, {pred_rle}\"\n        output_lines.append(output_line)\n\nwith open('submission.csv', \"w\") as f:\n    out = \"\\n\".join(line.strip() for line in output_lines)\n    f.write(out)","metadata":{"id":"QxheU3RPn_LJ","execution":{"iopub.status.busy":"2024-02-20T02:44:04.426479Z","iopub.status.idle":"2024-02-20T02:44:04.426789Z","shell.execute_reply.started":"2024-02-20T02:44:04.426634Z","shell.execute_reply":"2024-02-20T02:44:04.426649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"/kaggle/working/submission.csv\")","metadata":{"id":"T2l6S4YRn_LK","execution":{"iopub.status.busy":"2024-02-20T02:44:04.427939Z","iopub.status.idle":"2024-02-20T02:44:04.428276Z","shell.execute_reply.started":"2024-02-20T02:44:04.428112Z","shell.execute_reply":"2024-02-20T02:44:04.428128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font style=\"color:green\">6. Kaggle Profile Link [50 Points]</font>\n\nShare your Kaggle profile link here with us so that we can give points for the competition score.\n\nYou should have a minimum IoU of `0.60` on the test data to get all points. If the IoU is less than `0.55`, you will not get any points for the section.\n\n**You must have to submit `submission.csv` (prediction for images in `test.csv`) in `Submit Predictions` tab in Kaggle to get any evaluation in this section.**","metadata":{"id":"4vBqfZTMn_LK"}}]}